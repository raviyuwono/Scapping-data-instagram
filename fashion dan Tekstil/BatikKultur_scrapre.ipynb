{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e310e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from colorthief import ColorThief\n",
    "import webcolors\n",
    "import re\n",
    "\n",
    "# === Fungsi helper: RGB ke nama warna ===\n",
    "def closest_color(requested_color):\n",
    "    min_colors = {}\n",
    "    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():\n",
    "        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
    "        rd = (r_c - requested_color[0]) ** 2\n",
    "        gd = (g_c - requested_color[1]) ** 2\n",
    "        bd = (b_c - requested_color[2]) ** 2\n",
    "        min_colors[(rd + gd + bd)] = name\n",
    "    return min_colors[min(min_colors.keys())]\n",
    "\n",
    "def get_color_name(rgb_color):\n",
    "    try:\n",
    "        return closest_color(rgb_color)\n",
    "    except Exception:\n",
    "        return 'unknown'\n",
    "\n",
    "# === Fungsi untuk mengambil comments ===\n",
    "def get_comments_data(driver, soup):\n",
    "    comments_data = []\n",
    "    comments_count = 0\n",
    "    \n",
    "    # Metode 1: Cari berdasarkan struktur HTML yang lebih stabil\n",
    "    try:\n",
    "        # Cari semua span yang berisi teks comment\n",
    "        comment_spans = soup.find_all('span', dir='auto')\n",
    "        \n",
    "        for span in comment_spans:\n",
    "            # Skip jika span berisi username atau caption\n",
    "            parent = span.parent\n",
    "            if parent and parent.name == 'h1':  # Skip caption\n",
    "                continue\n",
    "                \n",
    "            text = span.get_text(strip=True)\n",
    "            if text and len(text) > 1:  # Filter teks yang valid\n",
    "                # Cek apakah ini bukan username (biasanya ada @ atau link)\n",
    "                if not text.startswith('@') and 'href' not in str(span):\n",
    "                    comments_data.append(text)\n",
    "                    comments_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error metode 1: {e}\")\n",
    "    \n",
    "    # Metode 2: Cari dengan XPath yang lebih fleksibel\n",
    "    try:\n",
    "        comment_elements = driver.find_elements(By.XPATH, \"//div[@role='button']//span[contains(@dir, 'auto')]\")\n",
    "        \n",
    "        for element in comment_elements:\n",
    "            text = element.text.strip()\n",
    "            if text and len(text) > 1 and text not in comments_data:\n",
    "                # Filter yang bukan username atau action\n",
    "                if not any(keyword in text.lower() for keyword in ['like', 'reply', 'follow', 'view']):\n",
    "                    comments_data.append(text)\n",
    "                    comments_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error metode 2: {e}\")\n",
    "    \n",
    "    # Metode 3: Cari berdasarkan aria-label\n",
    "    try:\n",
    "        comment_sections = soup.find_all('div', {'aria-label': re.compile(r'comment', re.I)})\n",
    "        for section in comment_sections:\n",
    "            spans = section.find_all('span')\n",
    "            for span in spans:\n",
    "                text = span.get_text(strip=True)\n",
    "                if text and len(text) > 1 and text not in comments_data:\n",
    "                    comments_data.append(text)\n",
    "                    comments_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error metode 3: {e}\")\n",
    "    \n",
    "    # Metode 4: Cari dengan pattern yang lebih umum\n",
    "    try:\n",
    "        # Cari elemen yang kemungkinan berisi comment\n",
    "        potential_comments = soup.find_all('span', string=re.compile(r'.{2,}'))\n",
    "        \n",
    "        for span in potential_comments:\n",
    "            text = span.get_text(strip=True)\n",
    "            parent_class = span.parent.get('class', []) if span.parent else []\n",
    "            \n",
    "            # Filter berdasarkan konteks\n",
    "            if (text and len(text) > 1 and \n",
    "                text not in comments_data and\n",
    "                not text.startswith('@') and\n",
    "                not any(keyword in text.lower() for keyword in ['like', 'reply', 'follow', 'view', 'ago', 'hour', 'day', 'week'])):\n",
    "                comments_data.append(text)\n",
    "                comments_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error metode 4: {e}\")\n",
    "    \n",
    "    return comments_data, comments_count\n",
    "\n",
    "# === Fungsi untuk mengambil likes dengan lebih akurat ===\n",
    "def get_likes_count(soup):\n",
    "    try:\n",
    "        # Metode 1: Cari berdasarkan pattern \"X likes\"\n",
    "        likes_pattern = soup.find('a', string=re.compile(r'\\d+.*like', re.I))\n",
    "        if likes_pattern:\n",
    "            likes_text = likes_pattern.get_text()\n",
    "            numbers = re.findall(r'[\\d,]+', likes_text)\n",
    "            return numbers[0] if numbers else ''\n",
    "        \n",
    "        # Metode 2: Cari berdasarkan struktur button\n",
    "        likes_button = soup.find('button', {'aria-label': re.compile(r'like', re.I)})\n",
    "        if likes_button:\n",
    "            likes_text = likes_button.get_text()\n",
    "            numbers = re.findall(r'[\\d,]+', likes_text)\n",
    "            return numbers[0] if numbers else ''\n",
    "        \n",
    "        # Metode 3: Cari span yang berisi angka + \"likes\"\n",
    "        all_spans = soup.find_all('span')\n",
    "        for span in all_spans:\n",
    "            text = span.get_text().lower()\n",
    "            if 'like' in text and any(char.isdigit() for char in text):\n",
    "                numbers = re.findall(r'[\\d,]+', text)\n",
    "                return numbers[0] if numbers else ''\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting likes: {e}\")\n",
    "    \n",
    "    return ''\n",
    "\n",
    "# === Setup Chrome dengan stabilitas yang lebih baik ===\n",
    "def create_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--disable-web-security\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-plugins\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    # Prefs untuk menghemat memory\n",
    "    prefs = {\n",
    "        \"profile.default_content_setting_values\": {\n",
    "            \"notifications\": 2,\n",
    "            \"media_stream\": 2,\n",
    "        }\n",
    "    }\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "# === Fungsi untuk restart driver jika error ===\n",
    "def restart_driver_if_needed(driver):\n",
    "    try:\n",
    "        driver.current_url  # Test jika driver masih hidup\n",
    "        return driver\n",
    "    except:\n",
    "        print(\"üîÑ Driver session bermasalah, restart driver...\")\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        return create_driver()\n",
    "\n",
    "driver = create_driver()\n",
    "\n",
    "# === Login manual ===\n",
    "driver.get('https://www.instagram.com/')\n",
    "print(\"üîë Silakan login manual dulu...\")\n",
    "time.sleep(40)\n",
    "\n",
    "# === Profil target ===\n",
    "username_target = 'batikkultur'\n",
    "profile_url = f'https://www.instagram.com/{username_target}/'\n",
    "driver.get(profile_url)\n",
    "time.sleep(5)\n",
    "\n",
    "# === Ambil kategori akun dari profil ===\n",
    "profile_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "try:\n",
    "    # Cari dengan berbagai metode\n",
    "    kategori_akun = ''\n",
    "    \n",
    "    # Metode 1: Cari berdasarkan struktur profile\n",
    "    category_divs = profile_soup.find_all('div', string=re.compile(r'Business|Shopping|Brand|Store|Art|Culture', re.I))\n",
    "    if category_divs:\n",
    "        kategori_akun = category_divs[0].get_text().strip()\n",
    "    \n",
    "    # Metode 2: Backup jika tidak ketemu\n",
    "    if not kategori_akun:\n",
    "        bio_sections = profile_soup.find_all('div', {'class': re.compile(r'_aa_c')})\n",
    "        for section in bio_sections:\n",
    "            text = section.get_text().strip()\n",
    "            if text and len(text) < 100:  # Kategori biasanya pendek\n",
    "                kategori_akun = text\n",
    "                break\n",
    "                \n",
    "except Exception as e:\n",
    "    kategori_akun = ''\n",
    "    print(f\"Error getting category: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Kategori akun: {kategori_akun}\")\n",
    "\n",
    "# === Scroll agar semua post muncul ===\n",
    "scroll_times = 50\n",
    "for _ in range(scroll_times):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "# === Ambil semua link post & reels ===\n",
    "post_links = []\n",
    "posts = driver.find_elements(By.XPATH, '//a[contains(@href, \"/p/\") or contains(@href, \"/reel/\")]')\n",
    "for post in posts:\n",
    "    href = post.get_attribute('href')\n",
    "    if href not in post_links:\n",
    "        post_links.append(href)\n",
    "\n",
    "print(f\"‚úÖ Total post ditemukan: {len(post_links)}\")\n",
    "\n",
    "# Batasi (misal ambil 100 post)\n",
    "post_links = post_links[:100]\n",
    "\n",
    "# === DATA OUTPUT ===\n",
    "data = []\n",
    "\n",
    "# === Loop setiap post dengan error handling ===\n",
    "for idx, link in enumerate(post_links):\n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            # Restart driver jika bermasalah\n",
    "            driver = restart_driver_if_needed(driver)\n",
    "            \n",
    "            driver.get(link)\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Tentukan jenis konten\n",
    "            if '/reel/' in link:\n",
    "                media_type = 'reel'\n",
    "            elif '/p/' in link:\n",
    "                media_type = 'post'\n",
    "            else:\n",
    "                media_type = ''\n",
    "\n",
    "            # Klik \"Muat komentar lainnya\" dengan lebih banyak variasi\n",
    "            load_more_attempts = 0\n",
    "            max_load_attempts = 10\n",
    "            \n",
    "            while load_more_attempts < max_load_attempts:\n",
    "                try:\n",
    "                    # Coba berbagai selector untuk tombol load more\n",
    "                    load_more_selectors = [\n",
    "                        \"//button[.//svg[@aria-label='Muat komentar lainnya']]\",\n",
    "                        \"//button[.//svg[@aria-label='Load more comments']]\",\n",
    "                        \"//button[contains(text(), 'View more comments')]\",\n",
    "                        \"//button[contains(text(), 'Lihat komentar lainnya')]\",\n",
    "                        \"//span[contains(text(), 'View more comments')]\",\n",
    "                        \"//span[contains(text(), 'Lihat komentar lainnya')]\"\n",
    "                    ]\n",
    "                    \n",
    "                    load_more_found = False\n",
    "                    for selector in load_more_selectors:\n",
    "                        try:\n",
    "                            load_more = driver.find_element(By.XPATH, selector)\n",
    "                            load_more.click()\n",
    "                            load_more_found = True\n",
    "                            time.sleep(2)\n",
    "                            break\n",
    "                        except NoSuchElementException:\n",
    "                            continue\n",
    "                    \n",
    "                    if not load_more_found:\n",
    "                        break\n",
    "                        \n",
    "                    load_more_attempts += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading more comments: {e}\")\n",
    "                    break\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Ambil data\n",
    "            brand = username_target\n",
    "\n",
    "            try:\n",
    "                # Cari caption dengan berbagai metode\n",
    "                caption = ''\n",
    "                \n",
    "                # Metode 1: Cari h1 dengan class spesifik\n",
    "                caption_h1 = soup.find('h1', class_='_ap3a _aaco _aacu _aacx _aad7 _aade')\n",
    "                if caption_h1:\n",
    "                    caption = caption_h1.get_text().strip()\n",
    "                \n",
    "                # Metode 2: Cari berdasarkan struktur yang lebih umum\n",
    "                if not caption:\n",
    "                    meta_desc = soup.find('meta', {'name': 'description'})\n",
    "                    if meta_desc:\n",
    "                        caption = meta_desc.get('content', '')\n",
    "                \n",
    "                # Metode 3: Cari span dengan teks panjang di area post\n",
    "                if not caption:\n",
    "                    spans = soup.find_all('span', dir='auto')\n",
    "                    for span in spans:\n",
    "                        text = span.get_text().strip()\n",
    "                        if len(text) > 20:  # Caption biasanya lebih panjang\n",
    "                            caption = text\n",
    "                            break\n",
    "                            \n",
    "            except Exception as e:\n",
    "                caption = ''\n",
    "                print(f\"Error getting caption: {e}\")\n",
    "\n",
    "            # Ambil likes dengan fungsi yang diperbaiki\n",
    "            likes = get_likes_count(soup)\n",
    "\n",
    "            # Ambil comments dengan fungsi yang diperbaiki\n",
    "            comments_data, comments_count = get_comments_data(driver, soup)\n",
    "\n",
    "            try:\n",
    "                media = soup.find('img')\n",
    "                if media:\n",
    "                    media_url = media.get('src', '')\n",
    "                else:\n",
    "                    video = soup.find('video')\n",
    "                    media_url = video.get('src', '') if video else ''\n",
    "            except:\n",
    "                media_url = ''\n",
    "\n",
    "            # Ambil dominant color\n",
    "            dominant_color = ''\n",
    "            color_name = ''\n",
    "            if media_url and media_type == 'post':\n",
    "                try:\n",
    "                    response = requests.get(media_url)\n",
    "                    img = BytesIO(response.content)\n",
    "                    ct = ColorThief(img)\n",
    "                    dominant_color = ct.get_color(quality=1)\n",
    "                    color_name = get_color_name(dominant_color)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting color: {e}\")\n",
    "                    dominant_color = ''\n",
    "                    color_name = ''\n",
    "\n",
    "            try:\n",
    "                upload_time = soup.find('time')['datetime']\n",
    "            except:\n",
    "                upload_time = ''\n",
    "\n",
    "            content_category = 'Batik Kultur post'\n",
    "\n",
    "            # Gabungkan semua comments menjadi satu string\n",
    "            all_comments = ' | '.join(comments_data) if comments_data else ''\n",
    "\n",
    "            data.append((\n",
    "                brand, link, caption, likes, comments_count, all_comments, media_url, media_type,\n",
    "                str(dominant_color), color_name, content_category, upload_time, kategori_akun\n",
    "            ))\n",
    "\n",
    "            print(f\"[{idx+1}] ‚úÖ {link} | {media_type} | Comments: {comments_count} | Likes: {likes}\")\n",
    "            print(f\"    üìù Comments preview: {all_comments[:100]}...\")\n",
    "            \n",
    "            # Sukses, keluar dari retry loop\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            print(f\"‚ùå Error pada post {idx+1} (retry {retry_count}/{max_retries}): {e}\")\n",
    "            \n",
    "            if retry_count < max_retries:\n",
    "                print(\"üîÑ Mencoba ulang...\")\n",
    "                time.sleep(10)  # Tunggu lebih lama sebelum retry\n",
    "            else:\n",
    "                print(f\"‚ùå Gagal setelah {max_retries} percobaan, skip post ini\")\n",
    "                # Tambahkan data kosong untuk post yang gagal\n",
    "                data.append((\n",
    "                    brand, link, '', '', 0, '', '', media_type,\n",
    "                    '', '', content_category, '', kategori_akun\n",
    "                ))\n",
    "    \n",
    "    time.sleep(5)  # Jeda antar post untuk stabilitas\n",
    "    \n",
    "\n",
    "# === Save ke Excel dengan error handling ===\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    'brand', 'url_post', 'caption', 'likes', 'comments_count', 'all_comments',\n",
    "    'media_url', 'media_type', 'dominant_color', 'color_name',\n",
    "    'content_category', 'upload_time', 'profile_category'\n",
    "])\n",
    "\n",
    "df.to_excel('konten_ig_Batikula_baru.xlsx', index=False)\n",
    "print(\"\\n‚úÖ Selesai! Data disimpan ke 'konten_ig_Batikula_baru.xlsx'\")\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
