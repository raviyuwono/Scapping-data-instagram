{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b11024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from colorthief import ColorThief\n",
    "import webcolors\n",
    "import re\n",
    "\n",
    "# === Fungsi helper: RGB ke nama warna ===\n",
    "def closest_color(requested_color):\n",
    "    min_colors = {}\n",
    "    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():\n",
    "        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
    "        rd = (r_c - requested_color[0]) ** 2\n",
    "        gd = (g_c - requested_color[1]) ** 2\n",
    "        bd = (b_c - requested_color[2]) ** 2\n",
    "        min_colors[(rd + gd + bd)] = name\n",
    "    return min_colors[min(min_colors.keys())]\n",
    "\n",
    "def get_color_name(rgb_color):\n",
    "    try:\n",
    "        return closest_color(rgb_color)\n",
    "    except Exception:\n",
    "        return 'unknown'\n",
    "\n",
    "# === Fungsi untuk mengambil comments ===\n",
    "def get_comments_data(driver, soup):\n",
    "    comments_data = []\n",
    "    comments_count = 0\n",
    "\n",
    "    try:\n",
    "        # Temukan semua komentar berdasarkan struktur baru\n",
    "        comment_spans = soup.find_all('span', {'dir': 'auto'})\n",
    "\n",
    "        for span in comment_spans:\n",
    "            text = span.get_text(strip=True)\n",
    "            # Filter komentar valid\n",
    "            if (\n",
    "                text and len(text) > 1\n",
    "                and not text.startswith('@')  # bukan mention\n",
    "                and not any(keyword in text.lower() for keyword in [\n",
    "                    'like', 'reply', 'view', 'follow', 'ago', 'hour', 'day', 'week',\n",
    "                    'menit', 'minute', 'disukai', 'balas'\n",
    "                ])\n",
    "            ):\n",
    "                comments_data.append(text)\n",
    "                comments_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting comments: {e}\")\n",
    "\n",
    "    return comments_data, comments_count\n",
    "\n",
    "\n",
    "# === Fungsi untuk mengambil likes dengan lebih akurat ===\n",
    "def get_likes_count(soup):\n",
    "    try:\n",
    "        # First check for \"No likes yet\" case\n",
    "        no_likes = soup.find(string=re.compile(r'No likes yet|Belum ada yang menyukai', re.I))\n",
    "        if no_likes:\n",
    "            return '0'\n",
    "            \n",
    "        # Method 1: Look for like count in meta description\n",
    "        meta_desc = soup.find('meta', {'name': 'description'})\n",
    "        if meta_desc:\n",
    "            desc = meta_desc.get('content', '')\n",
    "            match = re.search(r'(\\d+(?:,\\d+)*) Likes|(\\d+(?:,\\d+)*) suka', desc)\n",
    "            if match:\n",
    "                return match.group(1) or match.group(2)\n",
    "                \n",
    "        # Method 2: Look for like count in buttons\n",
    "        like_buttons = soup.find_all('button', string=re.compile(r'\\d+ Likes|\\d+ suka', re.I))\n",
    "        for button in like_buttons:\n",
    "            numbers = re.findall(r'[\\d,]+', button.get_text())\n",
    "            if numbers:\n",
    "                return numbers[0]\n",
    "                \n",
    "        # Method 3: Look for like count in spans\n",
    "        spans = soup.find_all('span')\n",
    "        for span in spans:\n",
    "            text = span.get_text()\n",
    "            if 'like' in text.lower() and any(char.isdigit() for char in text):\n",
    "                numbers = re.findall(r'[\\d,]+', text)\n",
    "                if numbers:\n",
    "                    return numbers[0]\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting likes: {e}\")\n",
    "    \n",
    "    return ''\n",
    "\n",
    "# === Setup Chrome ===\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--disable-notifications\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# === Login manual ===\n",
    "driver.get('https://www.instagram.com/')\n",
    "print(\"🔑 Silakan login manual dulu...\")\n",
    "time.sleep(40)\n",
    "\n",
    "# === Profil target ===\n",
    "username_target = 'danarhadi_id'\n",
    "profile_url = f'https://www.instagram.com/{username_target}/'\n",
    "driver.get(profile_url)\n",
    "time.sleep(5)\n",
    "\n",
    "# === Ambil kategori akun dari profil ===\n",
    "profile_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "try:\n",
    "    # Cari dengan berbagai metode\n",
    "    kategori_akun = ''\n",
    "    \n",
    "    # Metode 1: Cari berdasarkan struktur profile\n",
    "    category_divs = profile_soup.find_all('div', string=re.compile(r'Business|Shopping|Brand|Store', re.I))\n",
    "    if category_divs:\n",
    "        kategori_akun = category_divs[0].get_text().strip()\n",
    "    \n",
    "    # Metode 2: Backup jika tidak ketemu\n",
    "    if not kategori_akun:\n",
    "        bio_sections = profile_soup.find_all('div', {'class': re.compile(r'_aa_c')})\n",
    "        for section in bio_sections:\n",
    "            text = section.get_text().strip()\n",
    "            if text and len(text) < 100:  # Kategori biasanya pendek\n",
    "                kategori_akun = text\n",
    "                break\n",
    "                \n",
    "except Exception as e:\n",
    "    kategori_akun = ''\n",
    "    print(f\"Error getting category: {e}\")\n",
    "\n",
    "print(f\"✅ Kategori akun: {kategori_akun}\")\n",
    "\n",
    "# === Scroll agar semua post muncul ===\n",
    "scroll_times = 5\n",
    "for _ in range(scroll_times):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "# === Ambil semua link post & reels ===\n",
    "post_links = []\n",
    "posts = driver.find_elements(By.XPATH, '//a[contains(@href, \"/p/\") or contains(@href, \"/reel/\")]')\n",
    "for post in posts:\n",
    "    href = post.get_attribute('href')\n",
    "    if href not in post_links:\n",
    "        post_links.append(href)\n",
    "\n",
    "print(f\"✅ Total post ditemukan: {len(post_links)}\")\n",
    "\n",
    "# Batasi (misal ambil 100 post)\n",
    "post_links = post_links[:100]\n",
    "\n",
    "# === DATA OUTPUT ===\n",
    "data = []\n",
    "\n",
    "# === Loop setiap post ===\n",
    "for idx, link in enumerate(post_links):\n",
    "    driver.get(link)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Tentukan jenis konten\n",
    "    if '/reel/' in link:\n",
    "        media_type = 'reel'\n",
    "    elif '/p/' in link:\n",
    "        media_type = 'post'\n",
    "    else:\n",
    "        media_type = ''\n",
    "\n",
    "    # Klik \"Muat komentar lainnya\" dengan lebih banyak variasi\n",
    "    load_more_attempts = 0\n",
    "    max_load_attempts = 3\n",
    "    \n",
    "    while load_more_attempts < max_load_attempts:\n",
    "        try:\n",
    "            # Coba berbagai selector untuk tombol load more\n",
    "            load_more_selectors = [\n",
    "                \"//button[.//svg[@aria-label='Muat komentar lainnya']]\",\n",
    "                \"//button[.//svg[@aria-label='Load more comments']]\",\n",
    "                \"//button[contains(text(), 'View more comments')]\",\n",
    "                \"//button[contains(text(), 'Lihat komentar lainnya')]\",\n",
    "                \"//span[contains(text(), 'View more comments')]\",\n",
    "                \"//span[contains(text(), 'Lihat komentar lainnya')]\"\n",
    "            ]\n",
    "            \n",
    "            load_more_found = False\n",
    "            for selector in load_more_selectors:\n",
    "                try:\n",
    "                    load_more = driver.find_element(By.XPATH, selector)\n",
    "                    load_more.click()\n",
    "                    load_more_found = True\n",
    "                    time.sleep(2)\n",
    "                    break\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "            \n",
    "            if not load_more_found:\n",
    "                break\n",
    "                \n",
    "            load_more_attempts += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading more comments: {e}\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Ambil data\n",
    "    brand = username_target\n",
    "\n",
    "    try:\n",
    "        # Cari caption dengan berbagai metode\n",
    "        caption = ''\n",
    "        \n",
    "        # Metode 1: Cari h1 dengan class spesifik\n",
    "        caption_h1 = soup.find('h1', class_='_ap3a _aaco _aacu _aacx _aad7 _aade')\n",
    "        if caption_h1:\n",
    "            caption = caption_h1.get_text().strip()\n",
    "        \n",
    "        # Metode 2: Cari berdasarkan struktur yang lebih umum\n",
    "        if not caption:\n",
    "            meta_desc = soup.find('meta', {'name': 'description'})\n",
    "            if meta_desc:\n",
    "                caption = meta_desc.get('content', '')\n",
    "        \n",
    "        # Metode 3: Cari span dengan teks panjang di area post\n",
    "        if not caption:\n",
    "            spans = soup.find_all('span', dir='auto')\n",
    "            for span in spans:\n",
    "                text = span.get_text().strip()\n",
    "                if len(text) > 20:  # Caption biasanya lebih panjang\n",
    "                    caption = text\n",
    "                    break\n",
    "                    \n",
    "    except Exception as e:\n",
    "        caption = ''\n",
    "        print(f\"Error getting caption: {e}\")\n",
    "\n",
    "    # Ambil likes dengan fungsi yang diperbaiki\n",
    "    likes = get_likes_count(soup)\n",
    "\n",
    "    # Ambil comments dengan fungsi yang diperbaiki\n",
    "    comments_data, comments_count = get_comments_data(driver, soup)\n",
    "\n",
    "    try:\n",
    "        media = soup.find('img')\n",
    "        if media:\n",
    "            media_url = media.get('src', '')\n",
    "        else:\n",
    "            video = soup.find('video')\n",
    "            media_url = video.get('src', '') if video else ''\n",
    "    except:\n",
    "        media_url = ''\n",
    "\n",
    "    # Ambil dominant color\n",
    "    dominant_color = ''\n",
    "    color_name = ''\n",
    "    if media_url and media_type == 'post':\n",
    "        try:\n",
    "            response = requests.get(media_url)\n",
    "            img = BytesIO(response.content)\n",
    "            ct = ColorThief(img)\n",
    "            dominant_color = ct.get_color(quality=1)\n",
    "            color_name = get_color_name(dominant_color)\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting color: {e}\")\n",
    "            dominant_color = ''\n",
    "            color_name = ''\n",
    "\n",
    "    try:\n",
    "        upload_time = soup.find('time')['datetime']\n",
    "    except:\n",
    "        upload_time = ''\n",
    "\n",
    "    content_category = 'Batik Danar Hadi'\n",
    "\n",
    "    # Gabungkan semua comments menjadi satu string\n",
    "    all_comments = ' | '.join(comments_data) if comments_data else ''\n",
    "\n",
    "    data.append((\n",
    "        brand, link, caption, likes, comments_count, all_comments, media_url, media_type,\n",
    "        str(dominant_color), color_name, content_category, upload_time, kategori_akun\n",
    "    ))\n",
    "\n",
    "    print(f\"[{idx+1}] ✅ {link} | {media_type} | Comments: {comments_count} | Likes: {likes}\")\n",
    "    print(f\"    📝 Comments preview: {all_comments[:100]}...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# === Save ke Excel ===\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    'brand', 'url_post', 'caption', 'likes', 'comments_count', 'all_comments',\n",
    "    'media_url', 'media_type', 'dominant_color', 'color_name',\n",
    "    'content_category', 'upload_time', 'profile_category'\n",
    "])\n",
    "\n",
    "df.to_excel('konten_ig_BatikDanarHadi.xlsx', index=False)\n",
    "print(\"\\n✅ Selesai! Data disimpan ke 'konten_ig_BatikDanarHadi.xlsx'\")\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
